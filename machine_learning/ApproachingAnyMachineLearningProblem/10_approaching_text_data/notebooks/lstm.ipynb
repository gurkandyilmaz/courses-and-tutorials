{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-25T19:05:29.822937Z","iopub.execute_input":"2021-06-25T19:05:29.823291Z","iopub.status.idle":"2021-06-25T19:05:29.845490Z","shell.execute_reply.started":"2021-06-25T19:05:29.823258Z","shell.execute_reply":"2021-06-25T19:05:29.844389Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec\n/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:23:07.345445Z","iopub.execute_input":"2021-06-25T19:23:07.345822Z","iopub.status.idle":"2021-06-25T19:23:07.350717Z","shell.execute_reply.started":"2021-06-25T19:23:07.345792Z","shell.execute_reply":"2021-06-25T19:23:07.349464Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"DATA = \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\nFASTTEXT_EMBEDDINGS = \"../input/fasttext-wikinews/wiki-news-300d-1M.vec\"","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:05:29.859212Z","iopub.execute_input":"2021-06-25T19:05:29.859846Z","iopub.status.idle":"2021-06-25T19:05:29.871556Z","shell.execute_reply.started":"2021-06-25T19:05:29.859799Z","shell.execute_reply":"2021-06-25T19:05:29.870560Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def sentence_to_vec(sentence, embedding_dict={}, stop_words=[], tokenizer=None):\n    words = str(sentence).lower()\n    words = tokenizer(words)\n    words = [w for w in words if w not in stop_words]\n    words = [w for w in words if w.isalpha()]\n    \n    M = []\n    for w in words:\n        if w in embedding_dict:\n            M.append(embedding_dict[w])\n    \n    if len(M) == 0:\n        return np.zeros(300)\n    \n    v = np.array(M).sum(axis=0)\n    \n    return v / np.sqrt((v**2).sum())\n","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:05:29.873268Z","iopub.execute_input":"2021-06-25T19:05:29.873691Z","iopub.status.idle":"2021-06-25T19:05:29.884971Z","shell.execute_reply.started":"2021-06-25T19:05:29.873647Z","shell.execute_reply":"2021-06-25T19:05:29.884161Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import io\nfrom sklearn import linear_model, metrics, model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef load_vectors(fname):\n    # taken from: https://fasttext.cc/docs/en/english-vectors.html\n    fin = io.open(fname,'r',encoding='utf-8',newline='\\n',errors='ignore')\n    n, d = map(int, fin.readline().split())\n    data = {}\n    for line in fin:\n        tokens = line.rstrip().split(' ')\n        data[tokens[0]] = list(map(float, tokens[1:]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:05:29.886704Z","iopub.execute_input":"2021-06-25T19:05:29.887149Z","iopub.status.idle":"2021-06-25T19:05:29.905891Z","shell.execute_reply.started":"2021-06-25T19:05:29.887104Z","shell.execute_reply":"2021-06-25T19:05:29.904885Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\ndf = pd.read_csv(DATA, nrows=10000)\ndf[\"sentiment\"] = df[\"sentiment\"].apply(lambda label: 1 if label==\"positive\" else 0)\ndf = df.sample(frac=1.0).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:05:30.867748Z","iopub.execute_input":"2021-06-25T19:05:30.868121Z","iopub.status.idle":"2021-06-25T19:05:31.373754Z","shell.execute_reply.started":"2021-06-25T19:05:30.868082Z","shell.execute_reply":"2021-06-25T19:05:31.372693Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"CPU times: user 149 ms, sys: 29.3 ms, total: 178 ms\nWall time: 496 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nprint(\"Embeddings...\")\nembeddings = load_vectors(FASTTEXT_EMBEDDINGS)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:05:42.140014Z","iopub.execute_input":"2021-06-25T19:05:42.140405Z","iopub.status.idle":"2021-06-25T19:07:35.186515Z","shell.execute_reply.started":"2021-06-25T19:05:42.140374Z","shell.execute_reply":"2021-06-25T19:07:35.185562Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Embeddings...\n","output_type":"stream"}]},{"cell_type":"code","source":"from numpy import dot\nfrom numpy.linalg import norm\n\ngood = np.array(embeddings.get(\"good\"))\nbad = np.array(embeddings.get(\"bad\"))\nworst = np.array(embeddings.get(\"worst\"))\nbest = np.array(embeddings.get(\"best\"))\n\ngood_bad = dot(good, bad) / (norm(good)*norm(bad))\nprint(good_bad)\nbest_worst = dot(best, worst) / (norm(best)*norm(worst))\nprint(best_worst)","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:17:36.972958Z","iopub.execute_input":"2021-06-25T19:17:36.973365Z","iopub.status.idle":"2021-06-25T19:17:36.983126Z","shell.execute_reply.started":"2021-06-25T19:17:36.973331Z","shell.execute_reply":"2021-06-25T19:17:36.980511Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0.8331158890694513\n0.7301328920606268\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nprint(\"Sentence Vectors..\")\nvectors = []\nfor review in df[\"review\"].values:\n    vectors.append(\n        sentence_to_vec(\n            sentence = review,\n            embedding_dict = embeddings,\n            stop_words = [],\n            tokenizer = word_tokenize\n        )\n    )\n    \nvectors = np.array(vectors)\ny = df[\"sentiment\"].values\n\nkf = model_selection.StratifiedKFold(n_splits=3)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=vectors, y=y)):\n    print(\"FOLD: \", fold)\n    xtrain = vectors[train_idx, :]\n    xval = vectors[val_idx, :]\n    ytrain = y[train_idx]\n    yval = y[val_idx]\n    \n    model = linear_model.LogisticRegression()\n    model.fit(xtrain, ytrain)\n    preds = model.predict(xval)\n    acc = metrics.accuracy_score(yval, preds)\n    print(f\"Accuracy: {np.round(acc, 3)}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:17:47.352470Z","iopub.execute_input":"2021-06-25T19:17:47.353007Z","iopub.status.idle":"2021-06-25T19:19:51.731971Z","shell.execute_reply.started":"2021-06-25T19:17:47.352972Z","shell.execute_reply":"2021-06-25T19:19:51.730633Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Sentence Vectors..\nFOLD:  0\nAccuracy: 0.798\nFOLD:  1\nAccuracy: 0.824\nFOLD:  2\nAccuracy: 0.817\nCPU times: user 2min 4s, sys: 832 ms, total: 2min 5s\nWall time: 2min 4s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n#create_folds.py\ndef create_folds(df, num_folds=5):\n    df = df.sample(frac=1.0).reset_index(drop=True)\n    df[\"folds\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=num_folds)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df[\"review\"], y=df[\"sentiment\"])):\n        df.loc[val_idx, \"folds\"] = fold\n\n    return df\n\ndf = create_folds(df)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:27:20.610877Z","iopub.execute_input":"2021-06-25T19:27:20.611245Z","iopub.status.idle":"2021-06-25T19:27:20.636782Z","shell.execute_reply.started":"2021-06-25T19:27:20.611215Z","shell.execute_reply":"2021-06-25T19:27:20.635591Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"CPU times: user 9.51 ms, sys: 31 µs, total: 9.54 ms\nWall time: 10.9 ms\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                                                 review  sentiment  folds\n0     Sure it may not be a classic but it's one full...          1      0\n1     This might have been an excellent flick. Howev...          0      0\n2     This poor remake of the 1963 classic starts re...          0      0\n3     At school I was taught how some shots were cal...          1      0\n4     Farrah Fawcett gives an award nominated perfor...          1      0\n...                                                 ...        ...    ...\n9995  I mistakenly kept myself awake late last night...          0      4\n9996  Please! Do not waste any money on this movie. ...          0      4\n9997  I wanted to see the movie because of an articl...          0      4\n9998  If this movie is coming to a theater near you,...          0      4\n9999  The Internet Database lists this as a TV show....          1      4\n\n[10000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>folds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sure it may not be a classic but it's one full...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This might have been an excellent flick. Howev...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This poor remake of the 1963 classic starts re...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>At school I was taught how some shots were cal...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Farrah Fawcett gives an award nominated perfor...</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>I mistakenly kept myself awake late last night...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>Please! Do not waste any money on this movie. ...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>I wanted to see the movie because of an articl...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>If this movie is coming to a theater near you,...</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>The Internet Database lists this as a TV show....</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#dataset.py\nimport torch\n\nclass IMDBDataset:\n    def __init__(self, reviews, targets):\n        self.reviews = reviews\n        self.targets = targets\n        \n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = self.reviews[item, :]\n        target = self.targets[item]\n        return {\n            \"review\": torch.tensor(review, dtype=torch.long),\n            \"target\": torch.tensor(review, dtype=torch.float)\n        }\n","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:30:58.923896Z","iopub.execute_input":"2021-06-25T19:30:58.924328Z","iopub.status.idle":"2021-06-25T19:30:58.930852Z","shell.execute_reply.started":"2021-06-25T19:30:58.924297Z","shell.execute_reply":"2021-06-25T19:30:58.930014Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"### lstm.py\nimport torch.nn as nn\n\nclass LSTM(nn.Module):\n    def __init__(self, embedding_matrix):\n        super().__init__()\n        num_words = embedding_matrix.shape[0]\n        embedding_dim = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(num_embeddings=num_words, embedding_dim=embedding_dim)\n        \n        self.embedding.weight = nn.Parameter(\n            torch.tensor(embedding_matrix, dtype=torch.float32)\n        )\n        \n        self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(\n            embedding_dim,\n            128,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.out = nn.Linear(512,1)\n        \n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.lstm(x)\n        \n        avg_pool = torch.mean(x, 1)\n        max_pool = torch.max(x, 1)\n        \n        out = torch.cat((avg_pool, max_pool), 1)\n        out = self.out(out)\n        \n        return out\n        \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#engine.py\n\ndef train(data_loader, model, optimizer, device):\n    for data in data_loader:\n        reviews = data[\"review\"]\n        targets = data[\"target\"]\n        \n        reviews = reviews.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n        \n        predictions = model(reviews)\n        \n        loss = nn.BCEWithLogitsLoss(predictions, targets.view(-1,1))\n        \n        loss.backward()\n        optimizer.step()\n        \ndef evaluate(data_loader, model, device):\n    final_preds = []\n    final_targets = []\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for data in data_loader:\n            reviews = data[\"review\"]\n            targets = data[\"target\"]\n            reviews = reviews.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            \n            preds = model(reviews)\n            \n            preds = preds.cpu().numpy().tolist()\n            targets = data[\"target\"].cpu().numpy().tolist()\n            final_preds.extend(preds)\n            final_targets.extend(targets)\n            \n    return final_preds, final_targets","metadata":{"execution":{"iopub.status.busy":"2021-06-25T19:44:36.773808Z","iopub.execute_input":"2021-06-25T19:44:36.774229Z","iopub.status.idle":"2021-06-25T19:44:36.784733Z","shell.execute_reply.started":"2021-06-25T19:44:36.774184Z","shell.execute_reply":"2021-06-25T19:44:36.783719Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}